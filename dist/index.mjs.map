{"version":3,"sources":["../src/utils.ts","../src/api.ts","../src/client.ts","../src/types.ts"],"names":["ModelID"],"mappings":";AAUO,IAAM,WAAA,GAAc,MAAA;AAKpB,IAAM,aAAA,GAAgB,kBAAA;AACtB,IAAM,oBAAA,GAA2C;AAAA,EACtD,MAAA,EAAQ,aAAA;AAAA,EACR,IAAA,EAAM,WAAA;AAAA,EACN,OAAA,EAAS,IAAA;AAAA,EACT,OAAA,EAAS;AAAA;AACX;AACO,IAAM,wBAAA,GAAyC;AAAA,EAClD,MAAA,EAAQ,oBAAA;AAAA,EACR,GAAA,EAAK;AACT,CAAA;AAMO,IAAM,gBAAA,GAAmB,CAAC,MAAA,KAAuC;AACtE,EAAA,MAAM,EAAE,IAAA,EAAM,MAAA,EAAO,GAAI,MAAA;AACzB,EAAA,MAAM,OAAO,IAAA,IAAQ,WAAA;AACrB,EAAA,MAAM,MAAA,GAAS,MAAA,KAAW,SAAA,GAAY,aAAA,GAAgB,MAAA,IAAU,aAAA;AAChE,EAAA,MAAM,MAAA,GAAS,CAAA,EAAG,MAAM,CAAA,CAAA,EAAI,IAAI,CAAA,CAAA;AAChC,EAAA,OAAO,MAAA;AACT,CAAA;;;AChBO,IAAM,OAAA,GAAU,OAAO,EAAC,MAAA,EAAQ,QAAM,KAA4F;AACvI,EAAA,MAAM,OAAA,GAAU;AAAA,IACd,GAAI,MAAA,IAAU,EAAC,MAAA,EAAM;AAAA,IACrB,MAAA,EAAQ,KAAA;AAAA,IACR,OAAA,EAAS;AAAA,MACP,cAAA,EAAgB;AAAA;AAClB,GACF;AAEA,EAAA,IAAI;AACF,IAAA,MAAM,MAAA,GAAS,iBAAiB,MAAM,CAAA;AACtC,IAAA,MAAM,GAAA,GAAM,MAAM,KAAA,CAAM,CAAA,EAAG,MAAM,CAAA,CAAA,EAAI,MAAA,CAAO,OAAO,CAAA,QAAA,CAAA,EAAY,OAAO,CAAA;AACtE,IAAA,IAAI,CAAC,IAAI,EAAA,EAAI,MAAM,IAAI,KAAA,CAAM,CAAA,oBAAA,EAAuB,GAAA,CAAI,MAAM,CAAA,CAAE,CAAA;AAChE,IAAA,IAAI,CAAC,GAAA,EAAK,MAAM,IAAI,MAAM,uBAAuB,CAAA;AACjD,IAAA,OAAO,IAAI,IAAA,EAAK;AAAA,EAClB,SAAS,GAAA,EAAK;AACZ,IAAA,OAAA,CAAQ,KAAA,CAAM,kCAAkC,GAAG,CAAA;AACnD,IAAA,OAAO,IAAA;AAAA,EACT;AACF,CAAA;AAOO,IAAM,cAAA,GAAiB,CAC5B,MAAA,EACA,QAAA,KACyB;AACzB,EAAA,IAAI,CAAC,QAAA,IAAY,QAAA,CAAS,MAAA,KAAW,GAAG,OAAO,IAAA;AAE/C,EAAA,MAAM,cAAc,EAAC;AAGrB,EAAA,QAAA,CAAS,OAAA,CAAQ,CAAC,GAAA,KAAQ;AACxB,IAAA,MAAM,MAAA,GAAS,iBAAiB,MAAM,CAAA;AACtC,IAAA,MAAM,UAAU,GAAA,CAAI,IAAA;AACpB,IAAA,MAAM,YAIF,EAAC;AACL,IAAA,IAAI,GAAA;AAGJ,IAAA,GAAA,CAAI,SAAA,CAAU,OAAA,CAAQ,CAAC,QAAA,KAAa;AAElC,MAAA,MAAM,OAAA,GAAU,OACd,IAAA,KACG;AACH,QAAA,IAAI;AACF,UAAA,MAAM,WAAA,GAAc,EAAE,cAAA,EAAgB,kBAAA,EAAmB;AACzD,UAAA,MAAM,SAAS,QAAA,CAAS,MAAA;AACxB,UAAA,MAAM,OAAA,GAAU;AAAA,YACd,GAAI,MAAA,KAAW,MAAA,IAAU,CAAC,MAAM,QAAA,IAAY;AAAA,WAC9C;AACA,UAAA,MAAM,IAAA,GAAO,MAAM,QAAA,GACf,IAAA,CAAK,WACL,IAAA,CAAK,SAAA,CAAU,MAAM,IAAI,CAAA;AAC7B,UAAA,MAAM,SAAS,IAAA,EAAM,MAAA;AACrB,UAAA,MAAM,WAAA,GAAc,MAAM,WAAA,GACtB,IAAI,gBAAgB,IAAA,EAAM,WAAW,CAAA,CAAE,QAAA,EAAS,GAChD,IAAA;AACJ,UAAA,MAAM,QAAA,GAAW,WAAA,GAAc,CAAA,CAAA,EAAI,WAAW,CAAA,CAAA,GAAK,EAAA;AACnD,UAAA,MAAM,MAAM,CAAA,EAAG,MAAM,GAAG,QAAA,CAAS,OAAO,GAAG,QAAQ,CAAA,CAAA;AAEnD,UAAA,GAAA,GAAM,MAAM,MAAM,GAAA,EAAK;AAAA,YACrB,MAAA;AAAA,YACA,IAAA,EAAM,MAAA;AAAA;AAAA,YACN,KAAA,EAAO,UAAA;AAAA,YACP,WAAA,EAAa,aAAA;AAAA,YACb,OAAA;AAAA;AAAA,YACA,QAAA,EAAU,QAAA;AAAA,YACV,cAAA,EAAgB,aAAA;AAAA;AAAA,YAChB,IAAA;AAAA,YACA,GAAI,MAAA,IAAU,EAAE,MAAA;AAAO,WACxB,CAAA;AAGD,UAAA,IAAI,CAAC,GAAA;AACH,YAAA,MAAM,IAAI,KAAA,CAAM,CAAA,yBAAA,EAA4B,QAAA,CAAS,IAAI,CAAA,CAAA,CAAG,CAAA;AAG9D,UAAA,IAAI,CAAC,GAAA,EAAK,EAAA;AACR,YAAA,MAAM,IAAI,KAAA,CAAM,CAAA,sBAAA,EAAyB,GAAA,EAAK,UAAU,CAAA,CAAE,CAAA;AAG5D,UAAA,MAAM,YAAA,GAAe,GAAA,CAAI,OAAA,CAAQ,GAAA,CAAI,cAAc,CAAA;AACnD,UAAA,IAAI,IAAI,IAAA,IAAQ,CAAC,YAAA,EAAc,QAAA,CAAS,cAAc,CAAA,EAAG;AACvD,YAAA,MAAM,MAAA,GAAS,MAAM,GAAA,CAAI,IAAA,EAAK;AAE9B,YAAA,IAAI,CAAC,MAAA,EAAQ,MAAM,IAAI,MAAM,sBAAsB,CAAA;AAEnD,YAAA,IAAI,OAAO,MAAA,EAAQ,OAAA,KAAY,SAAA,IAAa,CAAC,MAAA,EAAQ,OAAA;AACnD,cAAA,MAAM,IAAI,KAAA;AAAA,gBACR,qCACE,QAAA,CAAS,IACX,eAAe,MAAA,EAAQ,OAAA,IAAW,QAAQ,MAAM,CAAA;AAAA,eAClD;AAEF,YAAA,OAAO,MAAA;AAAA,UACT;AAEA,UAAA,OAAO,GAAA;AAAA,QACT,SAAS,GAAA,EAAK;AACZ,UAAA,OAAA,CAAQ,KAAA,CAAM,CAAA,kBAAA,EAAqB,QAAA,CAAS,IAAI,MAAM,GAAG,CAAA;AACzD,UAAA,OAAO,EAAE,OAAA,EAAS,KAAA,EAAO,OAAA,EAAS,GAAA,EAAI;AAAA,QACxC;AAAA,MACF,CAAA;AAGA,MAAA,SAAA,CAAU,QAAA,CAAS,IAAI,CAAA,GAAI,OAAA;AAE3B,MAAA,SAAA,CAAU,OAAA,GAAU,GAAA,CAAI,OAAA,IAAW,EAAC;AAAA,IACtC,CAAC,CAAA;AAED,IAAA,WAAA,CAAY,OAAO,CAAA,GAAI,SAAA;AAAA,EACzB,CAAC,CAAA;AAED,EAAA,OAAO,WAAA;AACT,CAAA;AAMO,IAAM,cAAA,GAAiB,OAAO,MAAA,KAAuD;AAC1F,EAAA,MAAM,OAAA,GAAU;AAAA,IACd,MAAA,EAAQ,KAAA;AAAA,IACR,OAAA,EAAS;AAAA,MACP,cAAA,EAAgB;AAAA;AAClB,GACF;AAEA,EAAA,IAAI;AACF,IAAA,MAAM,QAAA,GAAW,iCAAA;AACjB,IAAA,MAAM,GAAA,GAAM,iBAAiB,MAAM,CAAA;AACnC,IAAA,MAAM,GAAA,GAAM,MAAM,KAAA,CAAM,CAAA,EAAG,GAAG,CAAA,EAAG,QAAQ,IAAI,OAAO,CAAA;AACpD,IAAA,IAAI,CAAC,IAAI,EAAA,EAAI,MAAM,IAAI,KAAA,CAAM,CAAA,oBAAA,EAAuB,GAAA,CAAI,MAAM,CAAA,CAAE,CAAA;AAChE,IAAA,IAAI,CAAC,GAAA,EAAK,MAAM,IAAI,KAAA,CAAM,CAAA,iBAAA,EAAoB,QAAQ,CAAA,CAAE,CAAA;AACxD,IAAA,MAAM,MAAA,GAA6B,MAAM,GAAA,CAAI,IAAA,EAAK;AAClD,IAAA,MAAM,UAAU,MAAA,EAAQ,OAAA;AACxB,IAAA,IAAI,CAAC,SAAS,OAAO,IAAA;AACrB,IAAA,MAAM,OAAO,MAAA,CAAO,IAAA;AACpB,IAAA,OAAO,IAAA;AAAA,EACT,SAAS,GAAA,EAAK;AACZ,IAAA,OAAA,CAAQ,KAAA,CAAM,iCAAiC,GAAG,CAAA;AAClD,IAAA,OAAO,IAAA;AAAA,EACT;AACF,CAAA;;;ACjKA,IAAM,cAAN,MAAkB;AAAA,EAAlB,WAAA,GAAA;AACE,IAAA,IAAA,CAAQ,YAAA,GAAe,KAAA;AACvB,IAAA,IAAA,CAAQ,eAAA,GAA0C,IAAA;AAClD,IAAA,IAAA,CAAQ,UAAA,GAA2B,wBAAA;AAAA,EAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOnC,WAAA,GAAuB;AACrB,IAAA,OAAO,IAAA,CAAK,gBAAgB,CAAC,CAAC,KAAK,UAAA,CAAW,GAAA,IAAO,IAAA,CAAK,UAAA,CAAW,MAAA,CAAO,OAAA;AAAA,EAC9E;AAAA;AAAA;AAAA;AAAA,EAKA,aAAA,GAA8B;AAC5B,IAAA,OAAO,IAAA,CAAK,UAAA;AAAA,EACd;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,OAAA,CAAQ,EAAC,MAAA,EAAQ,QAAM,EAAyE;AACpG,IAAA,IAAI,KAAK,YAAA,EAAc;AACnB,MAAA,OAAA,CAAQ,IAAI,uCAAuC,CAAA;AACnD,MAAA,OAAO,KAAA;AAAA,IACX;AACA,IAAA,IAAI;AAEF,MAAA,MAAM,WAAA,GAAc,MAAM,OAAA,CAAQ,EAAC,MAAA,EAAQ,GAAI,MAAA,IAAU,EAAC,MAAA,EAAM,EAAG,CAAA;AACnE,MAAA,IAAI,CAAC,WAAA,EAAa,OAAA,QAAe,IAAI,KAAA,CAAM,aAAa,OAAO,CAAA;AAE/D,MAAA,MAAM,SAAA,GAAY,MAAM,cAAA,CAAe,MAAM,CAAA;AAC7C,MAAA,IAAI,CAAC,SAAA,EAAW,MAAM,IAAI,MAAM,kBAAkB,CAAA;AAClD,MAAA,MAAM,WAAA,GAAc,cAAA,CAAe,MAAA,EAAQ,SAAS,CAAA;AAEpD,MAAA,IAAI,WAAA,EAAa;AACf,QAAA,IAAA,CAAK,YAAA,GAAe,IAAA;AACpB,QAAA,OAAA,CAAQ,IAAI,6CAA6C,CAAA;AAEzD,QAAA,IAAA,CAAK,UAAA,GAAa,EAAC,MAAA,EAAQ,GAAA,EAAK,WAAA,EAAW;AAC3C,QAAA,OAAO,IAAA;AAAA,MACT;AAEA,MAAA,OAAO,KAAA;AAAA,IACT,SAAS,KAAA,EAAO;AACd,MAAA,OAAA,CAAQ,KAAA,CAAM,uCAAuC,KAAK,CAAA;AAC1D,MAAA,IAAA,CAAK,YAAA,GAAe,KAAA;AACpB,MAAA,OAAO,KAAA;AAAA,IACT;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAM,IAAA,CACJ,OAAA,GAAU,GAAA,EAKT;AAGD,IAAA,MAAM,UAAA,GAAa,IAAI,eAAA,EAAgB;AACvC,IAAA,MAAM,YAAY,UAAA,CAAW,MAAM,UAAA,CAAW,KAAA,IAAS,OAAO,CAAA;AAC9D,IAAA,MAAM,SAAA,GAAY,YAAY,GAAA,EAAI;AAElC,IAAA,IAAI;AACF,MAAA,MAAM,WAAA,GAAc,MAAM,OAAA,CAAQ,EAAC,MAAA,EAAQ,IAAA,CAAK,UAAA,CAAW,MAAA,EAAQ,MAAA,EAAQ,UAAA,CAAW,MAAA,EAAO,CAAA;AAC7F,MAAA,YAAA,CAAa,SAAS,CAAA;AAEtB,MAAA,IAAI,CAAC,WAAA,EAAa,OAAA,QAAe,IAAI,KAAA,CAAM,aAAa,OAAO,CAAA;AAC/D,MAAA,OAAO,EAAE,OAAA,EAAS,IAAA,EAAM,YAAA,EAAc,IAAA,CAAK,MAAM,WAAA,CAAY,GAAA,EAAI,GAAI,SAAS,CAAA,EAAE;AAAA,IAClF,SAAS,KAAA,EAAO;AACd,MAAA,YAAA,CAAa,SAAS,CAAA;AACtB,MAAA,OAAO;AAAA,QACL,OAAA,EAAS,KAAA;AAAA,QACT,KAAA,EAAO,KAAA,YAAiB,KAAA,GAAQ,KAAA,CAAM,OAAA,GAAU;AAAA,OAClD;AAAA,IACF;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKA,aAAA,GAAsB;AACpB,IAAA,IAAI,KAAK,eAAA,EAAiB;AACxB,MAAA,IAAA,CAAK,gBAAgB,KAAA,EAAM;AAC3B,MAAA,IAAA,CAAK,eAAA,GAAkB,IAAA;AAAA,IACzB;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKA,UAAA,GAAmB;AACjB,IAAA,IAAA,CAAK,aAAA,EAAc;AACnB,IAAA,IAAA,CAAK,WAAW,GAAA,GAAM,IAAA;AACtB,IAAA,IAAA,CAAK,YAAA,GAAe,KAAA;AAAA,EACtB;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAc,wBAAwB,QAAA,EAAqC;AACzE,IAAA,MAAM,MAAA,GAAS,QAAA,CAAS,IAAA,EAAM,SAAA,EAAU;AACxC,IAAA,IAAI,CAAC,MAAA,EAAQ;AACX,MAAA,MAAM,IAAI,MAAM,4CAA4C,CAAA;AAAA,IAC9D;AAEA,IAAA,MAAM,OAAA,GAAU,IAAI,WAAA,EAAY;AAChC,IAAA,IAAI,QAAA,GAAW,EAAA;AAEf,IAAA,IAAI;AAEF,MAAA,OAAO,IAAA,EAAM;AACX,QAAA,MAAM,EAAE,IAAA,EAAM,KAAA,EAAM,GAAI,MAAM,OAAO,IAAA,EAAK;AAC1C,QAAA,IAAI,IAAA,EAAM;AAEV,QAAA,MAAM,QAAQ,OAAA,CAAQ,MAAA,CAAO,OAAO,EAAE,MAAA,EAAQ,MAAM,CAAA;AAEpD,QAAA,MAAM,KAAA,GAAQ,KAAA,CAAM,KAAA,CAAM,IAAI,CAAA;AAC9B,QAAA,KAAA,MAAW,QAAQ,KAAA,EAAO;AACxB,UAAA,IAAI,IAAA,CAAK,UAAA,CAAW,QAAQ,CAAA,EAAG;AAC7B,YAAA,MAAM,IAAA,GAAO,IAAA,CAAK,KAAA,CAAM,CAAC,CAAA;AACzB,YAAA,IAAI,SAAS,QAAA,EAAU;AACvB,YAAA,IAAI;AACF,cAAA,MAAM,MAAA,GAAS,IAAA,CAAK,KAAA,CAAM,IAAI,CAAA;AAC9B,cAAA,QAAA,IAAY,IAAA,CAAK,wBAAwB,MAAM,CAAA;AAAA,YACjD,SAAS,CAAA,EAAG;AAAA,YAEZ;AAAA,UACF;AAAA,QACF;AAAA,MACF;AAAA,IACF,CAAA,SAAE;AACA,MAAA,MAAA,CAAO,WAAA,EAAY;AAAA,IACrB;AAEA,IAAA,OAAO,QAAA;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKQ,wBAAwB,QAAA,EAAuB;AAErD,IAAA,IAAI,SAAS,IAAA,EAAM;AACjB,MAAA,OAAO,QAAA,CAAS,IAAA;AAAA,IAClB;AAGA,IAAA,IAAI,SAAS,QAAA,EAAU;AACrB,MAAA,OAAO,QAAA,CAAS,QAAA;AAAA,IAClB;AAGA,IAAA,IAAI,QAAA,CAAS,IAAA,IAAQ,OAAO,QAAA,CAAS,SAAS,QAAA,EAAU;AACtD,MAAA,OAAO,QAAA,CAAS,IAAA;AAAA,IAClB;AAGA,IAAA,IAAI,SAAS,OAAA,IAAW,KAAA,CAAM,OAAA,CAAQ,QAAA,CAAS,OAAO,CAAA,EAAG;AACvD,MAAA,OACE,QAAA,CAAS,OAAA,CAAQ,CAAC,CAAA,EAAG,IAAA,IAAQ,SAAS,OAAA,CAAQ,CAAC,CAAA,EAAG,OAAA,EAAS,OAAA,IAAW,EAAA;AAAA,IAE1E;AAGA,IAAA,OAAO,OAAO,QAAQ,CAAA;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAM,WAAA,CACJ,QAAA,EACA,OAAA,EACiB;AACjB,IAAA,IAAI,CAAC,IAAA,CAAK,WAAA,EAAY,EAAG;AACvB,MAAA,MAAM,IAAI,MAAM,gCAAgC,CAAA;AAAA,IAClD;AAGA,IAAA,IAAA,CAAK,eAAA,GAAkB,IAAI,eAAA,EAAgB;AAE3C,IAAA,IAAI;AACF,MAAA,MAAM,WAAW,MAAM,IAAA,CAAK,UAAA,EAAY,GAAA,EAAK,cAAc,QAAA,CAAS;AAAA,QAClE,IAAA,EAAM;AAAA,UACJ,QAAA;AAAA,UACA,YAAA,EAAc,MAAA;AAAA,UACd,WAAA,EAAa,GAAA;AAAA,UACb,UAAA,EAAY,IAAA;AAAA,UACZ,MAAA,EAAQ,KAAA;AAAA;AAAA,UACR,GAAG;AAAA,SACL;AAAA,QACA,MAAA,EAAQ,KAAK,eAAA,CAAgB;AAAA,OAC9B,CAAA;AAED,MAAA,IAAI,CAAC,QAAA,EAAU;AACb,QAAA,MAAM,IAAI,MAAM,6BAA6B,CAAA;AAAA,MAC/C;AAGA,MAAA,IAAI,OAAO,aAAa,QAAA,EAAU;AAChC,QAAA,OAAO,QAAA;AAAA,MACT;AAIA,MAAA,IACE,OAAO,aAAa,QAAA,IACpB,QAAA,KAAa,QACb,SAAA,IAAa,QAAA,IACb,UAAU,QAAA,EACV;AACA,QAAA,MAAM,YAAA,GAAe,QAAA;AACrB,QAAA,MAAM,WAAA,GAAc,YAAA,CAAa,OAAA,CAAQ,GAAA,CAAI,cAAc,CAAA;AAC3D,QAAA,IAAI,WAAA,EAAa,QAAA,CAAS,cAAc,CAAA,EAAG;AAEzC,UAAA,OAAO,MAAM,IAAA,CAAK,uBAAA,CAAwB,YAAY,CAAA;AAAA,QACxD,CAAA,MAAO;AAEL,UAAA,MAAM,IAAA,GAAO,MAAM,YAAA,CAAa,IAAA,EAAK;AACrC,UAAA,OAAO,IAAA,CAAK,wBAAwB,IAAI,CAAA;AAAA,QAC1C;AAAA,MACF;AAGA,MAAA,OAAO,IAAA,CAAK,wBAAwB,QAAQ,CAAA;AAAA,IAC9C,SAAS,KAAA,EAAO;AACd,MAAA,IAAI,KAAA,YAAiB,KAAA,IAAS,KAAA,CAAM,IAAA,KAAS,YAAA,EAAc;AACzD,QAAA,MAAM,IAAI,MAAM,uBAAuB,CAAA;AAAA,MACzC;AACA,MAAA,MAAM,KAAA;AAAA,IACR;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,SAAA,CAAU,SAAA,EAAmB,OAAA,EAAmC;AACpE,IAAA,IAAI,CAAC,IAAA,CAAK,WAAA,EAAY,EAAG;AACvB,MAAA,MAAM,IAAI,MAAM,gCAAgC,CAAA;AAAA,IAClD;AAEA,IAAA,IAAI;AACF,MAAA,MAAM,IAAA,CAAK,UAAA,EAAY,GAAA,EAAK,aAAA,CAAc,IAAA,CAAK;AAAA,QAC7C,IAAA,EAAM;AAAA,UACJ,SAAA;AAAA,UACA,OAAA;AAAA,UACA,IAAA,EAAM;AAAA,YACJ,KAAA,EAAO,IAAA;AAAA,YACP,SAAA,EAAW,CAAA;AAAA,YACX,YAAA,EAAc,CAAA;AAAA,WAChB;AAAA,UACA,IAAA,EAAM;AAAA,YACJ,WAAA,EAAa,GAAA;AAAA,YACb,UAAA,EAAY;AAAA;AACd;AACF,OACD,CAAA;AACD,MAAA,OAAO,IAAA;AAAA,IACT,SAAS,KAAA,EAAO;AACd,MAAA,OAAA,CAAQ,KAAA,CAAM,yBAAyB,KAAK,CAAA;AAC5C,MAAA,OAAO,KAAA;AAAA,IACT;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAAA,GAAiB;AACrB,IAAA,IAAI,CAAC,IAAA,CAAK,WAAA,EAAY,EAAG;AACvB,MAAA,OAAO,IAAA;AAAA,IACT;AAEA,IAAA,IAAI;AACF,MAAA,MAAM,WAAW,MAAM,IAAA,CAAK,UAAA,EAAY,GAAA,EAAK,cAAc,KAAA,EAAM;AACjE,MAAA,OAAO,UAAU,IAAA,IAAQ,IAAA;AAAA,IAC3B,SAAS,KAAA,EAAO;AACd,MAAA,OAAA,CAAQ,KAAA,CAAM,+BAA+B,KAAK,CAAA;AAClD,MAAA,OAAO,IAAA;AAAA,IACT;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,kBAAA,GAAqB;AACzB,IAAA,IAAI,CAAC,IAAA,CAAK,WAAA,EAAY,EAAG;AACvB,MAAA,OAAO,EAAC;AAAA,IACV;AAEA,IAAA,IAAI;AACF,MAAA,MAAM,WAAW,MAAM,IAAA,CAAK,UAAA,EAAY,GAAA,EAAK,cAAc,SAAA,EAAU;AACrE,MAAA,OAAO,QAAA,EAAU,QAAQ,EAAC;AAAA,IAC5B,SAAS,KAAA,EAAO;AACd,MAAA,OAAA,CAAQ,KAAA,CAAM,mCAAmC,KAAK,CAAA;AACtD,MAAA,OAAO,EAAC;AAAA,IACV;AAAA,EACF;AACF,CAAA;AAGO,IAAM,WAAA,GAAc,IAAI,WAAA;;;AC5RxB,IAAK,OAAA,qBAAAA,QAAAA,KAAL;AACL,EAAAA,SAAA,MAAA,CAAA,GAAO,QAAA;AACP,EAAAA,SAAA,MAAA,CAAA,GAAO,MAAA;AACP,EAAAA,SAAA,QAAA,CAAA,GAAS,eAAA;AACT,EAAAA,SAAA,QAAA,CAAA,GAAS,QAAA;AACT,EAAAA,SAAA,OAAA,CAAA,GAAQ,OAAA;AALE,EAAA,OAAAA,QAAAA;AAAA,CAAA,EAAA,OAAA,IAAA,EAAA;AAsFL,IAAM,yBAAA,GAA4B;AAClC,IAAM,0BAAA,GAA6B;AACnC,IAAM,qBAAA,GAAwB;AAC9B,IAAM,0BAAA,GAA6B;AACnC,IAAM,sBAAA,GAAyB;AAC/B,IAAM,wBAAA,GAA2B;AACjC,IAAM,eAAA,GAAkB;AACxB,IAAM,kBAAA,GAAqB;AAC3B,IAAM,qBAAA,GAAwB","file":"index.mjs","sourcesContent":["/**\n * Utility functions for Obrew API\n * @module utils\n */\n\nimport { I_ConnectionConfig, I_Connection } from \"./types\";\n\n/**\n * Default port for Obrew API\n */\nexport const defaultPort = \"8008\";\n/**\n * Default domain for Obrew API\n * @TODO We need a method to tell whether the app is currently running locally (dev-mode) or hosted on server (web).\n */\nexport const defaultDomain = \"http://localhost\";\nexport const DEFAULT_OBREW_CONFIG: I_ConnectionConfig = {\n  domain: defaultDomain,\n  port: defaultPort,\n  version: 'v1',\n  enabled: false, // Disabled by default until connected\n}\nexport const DEFAULT_OBREW_CONNECTION: I_Connection = {\n    config: DEFAULT_OBREW_CONFIG,\n    api: null\n}\n\n/**\n * Create a fully qualified domain name from stored connection settings\n * @returns The complete origin URL (e.g., \"http://localhost:8008\")\n */\nexport const createDomainName = (config: I_ConnectionConfig): string => {\n  const { port, domain } = config;\n  const PORT = port || defaultPort;\n  const DOMAIN = domain === \"0.0.0.0\" ? defaultDomain : domain || defaultDomain;\n  const origin = `${DOMAIN}:${PORT}`;\n  return origin;\n};\n","/**\n * Core API client for Obrew backend services\n * @module api\n */\n\nimport { createDomainName } from \"./utils\";\nimport type {\n  I_API,\n  I_ConnectResponse,\n  I_ServicesResponse,\n  I_ServiceApis,\n  I_GenericAPIRequestParams,\n  T_GenericReqPayload,\n  T_APIConfigOptions,\n  I_ConnectionConfig,\n} from \"./types\";\n\n/**\n * Connect to the Obrew backend server\n * @returns A promise that resolves with connection info or null on failure\n */\nexport const connect = async ({config, signal}:{config: I_ConnectionConfig, signal?: AbortSignal}): Promise<I_ConnectResponse | null> => {\n  const options = {\n    ...(signal && {signal}),\n    method: \"GET\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n    },\n  };\n\n  try {\n    const origin = createDomainName(config);\n    const res = await fetch(`${origin}/${config.version}/connect`, options);\n    if (!res.ok) throw new Error(`HTTP error! Status: ${res.status}`);\n    if (!res) throw new Error(\"No response received.\");\n    return res.json();\n  } catch (err) {\n    console.error(\"[obrew] connectToServer error:\", err);\n    return null;\n  }\n};\n\n/**\n * Create service API clients from backend configuration\n * @param response - The API configuration from the backend\n * @returns Service API clients or null if configuration is invalid\n */\nexport const createServices = (\n  config: I_ConnectionConfig,\n  response: I_API[] | null\n): I_ServiceApis | null => {\n  if (!response || response.length === 0) return null;\n\n  const serviceApis = {} as I_ServiceApis;\n\n  // Construct api funcs for each service\n  response.forEach((api) => {\n    const origin = createDomainName(config);\n    const apiName = api.name;\n    const endpoints: {\n      [key: string]: (args: any) => Promise<Response | null>;\n    } & {\n      configs?: T_APIConfigOptions;\n    } = {};\n    let res: Response;\n\n    // Parse endpoint urls\n    api.endpoints.forEach((endpoint) => {\n      // Create a curried fetch function\n      const request = async (\n        args: I_GenericAPIRequestParams<T_GenericReqPayload>\n      ) => {\n        try {\n          const contentType = { \"Content-Type\": \"application/json\" };\n          const method = endpoint.method;\n          const headers = {\n            ...(method === \"POST\" && !args?.formData && contentType),\n          };\n          const body = args?.formData\n            ? args.formData\n            : JSON.stringify(args?.body);\n          const signal = args?.signal;\n          const queryParams = args?.queryParams\n            ? new URLSearchParams(args?.queryParams).toString()\n            : null;\n          const queryUrl = queryParams ? `?${queryParams}` : \"\";\n          const url = `${origin}${endpoint.urlPath}${queryUrl}`;\n\n          res = await fetch(url, {\n            method,\n            mode: \"cors\", // no-cors, *, cors, same-origin\n            cache: \"no-cache\",\n            credentials: \"same-origin\",\n            headers, // { 'Content-Type': 'multipart/form-data' }, // Browser will set this automatically for us for \"formData\"\n            redirect: \"follow\",\n            referrerPolicy: \"no-referrer\", // no-referrer, *no-referrer-when-downgrade, origin, origin-when-cross-origin, same-origin, strict-origin, strict-origin-when-cross-origin, unsafe-url\n            body,\n            ...(signal && { signal }),\n          });\n\n          // Check no response\n          if (!res)\n            throw new Error(`No response for endpoint ${endpoint.name}.`);\n\n          // Check bad request\n          if (!res?.ok)\n            throw new Error(`Something went wrong. ${res?.statusText}`);\n\n          // Check json response\n          const responseType = res.headers.get(\"content-type\");\n          if (res.json && !responseType?.includes(\"event-stream\")) {\n            const result = await res.json();\n\n            if (!result) throw new Error(\"Something went wrong\");\n            // Check failure from obrew api\n            if (typeof result?.success === \"boolean\" && !result?.success)\n              throw new Error(\n                `An unexpected error occurred for [${\n                  endpoint.name\n                }] endpoint: ${result?.message ?? result?.detail}`\n              );\n            // Success\n            return result;\n          }\n          // Return raw response (for streaming)\n          return res;\n        } catch (err) {\n          console.error(`[obrew] Endpoint \"${endpoint.name}\":`, err);\n          return { success: false, message: err };\n        }\n      };\n\n      // Add request function for this endpoint\n      endpoints[endpoint.name] = request;\n      // Set api configs\n      endpoints.configs = api.configs || {};\n    });\n    // Set api callbacks\n    serviceApis[apiName] = endpoints;\n  });\n\n  return serviceApis;\n};\n\n/**\n * Get the API configuration from the backend\n * @returns A promise that resolves with the API array or null on failure\n */\nexport const fetchAPIConfig = async (config:I_ConnectionConfig): Promise<I_API[] | null> => {\n  const options = {\n    method: \"GET\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n    },\n  };\n\n  try {\n    const endpoint = \"/${config.version}/services/api\";\n    const url = createDomainName(config);\n    const res = await fetch(`${url}${endpoint}`, options);\n    if (!res.ok) throw new Error(`HTTP error! Status: ${res.status}`);\n    if (!res) throw new Error(`No response from ${endpoint}`);\n    const result: I_ServicesResponse = await res.json();\n    const success = result?.success;\n    if (!success) return null;\n    const apis = result.data;\n    return apis;\n  } catch (err) {\n    console.error(\"[obrew] fetchAPIConfig error:\", err);\n    return null;\n  }\n};\n","import { createServices, fetchAPIConfig, connect } from \"./api\";\nimport { I_Connection, I_ConnectionConfig, Message, I_InferenceGenerateOptions } from \"./types\";\nimport { DEFAULT_OBREW_CONNECTION } from \"./utils\";\n\n/**\n * ObrewClient responsibilities:\n * 1. Handle connections and server config (track host/port in mem)\n * 2. Provide wrapper functions around obrew api\n * 3. Handle teardown/cleanup of network calls, etc when client unmounts/disconnects\n */\nclass ObrewClient {\n  private hasConnected = false\n  private abortController: AbortController | null = null\n  private connection: I_Connection = DEFAULT_OBREW_CONNECTION\n\n  // Data Methods //\n  \n  /**\n  * Check if service is connected\n  */\n  isConnected(): boolean {\n    return this.hasConnected && !!this.connection.api && this.connection.config.enabled\n  }\n\n  /**\n   * Return the current connection\n   */\n  getConnection(): I_Connection {\n    return this.connection\n  }\n\n  // Connection Methods //\n\n  /**\n  * Initialize connection to Obrew backend.\n  */\n  async connect({config, signal}: {config: I_ConnectionConfig, signal?: AbortSignal}): Promise<boolean> {\n    if (this.hasConnected) {\n        console.log('[obrew] Connection is already active!')\n        return false\n    }\n    try {\n      // Attempt handshake connection\n      const connSuccess = await connect({config, ...(signal && {signal})})\n      if (!connSuccess?.success) throw new Error(connSuccess?.message);\n      // Get API configuration and create services\n      const apiConfig = await fetchAPIConfig(config)\n      if (!apiConfig) throw new Error(\"No api returned.\");\n      const serviceApis = createServices(config, apiConfig)\n\n      if (serviceApis) {\n        this.hasConnected = true\n        console.log('[obrew] Successfully connected to Obrew API')\n        // Store config in connection after successful connect\n        this.connection = {config, api: serviceApis}\n        return true\n      }\n\n      return false\n    } catch (error) {\n      console.error('[obrew] Failed to connect to Obrew:', error)\n      this.hasConnected = false\n      return false\n    }\n  }\n\n  /**\n  * Ping server to check if it's responsive.\n  * Used for server discovery and health checks.\n  */\n  async ping(\n    timeout = 5000\n  ): Promise<{\n    success: boolean\n    responseTime?: number\n    error?: string\n  }> {\n    // const {domain: url, port} = this.connection.config\n    // const endpointHealth = `${url}:${port}/api/health`\n    const controller = new AbortController()\n    const timeoutId = setTimeout(() => controller.abort(), timeout)\n    const startTime = performance.now()\n\n    try {\n      const connSuccess = await connect({config: this.connection.config, signal: controller.signal})\n      clearTimeout(timeoutId)\n      // Check\n      if (!connSuccess?.success) throw new Error(connSuccess?.message);\n      return { success: true, responseTime: Math.round(performance.now() - startTime) }      \n    } catch (error) {\n      clearTimeout(timeoutId)\n      return {\n        success: false,\n        error: error instanceof Error ? error.message : 'Connection failed',\n      }\n    }\n  }\n\n  /**\n  * Cancel ongoing request\n  */\n  cancelRequest(): void {\n    if (this.abortController) {\n      this.abortController.abort()\n      this.abortController = null\n    }\n  }\n\n  /**\n  * Disconnect from service\n  */\n  disconnect(): void {\n    this.cancelRequest()\n    this.connection.api = null\n    this.hasConnected = false\n  }\n\n  // Core API Helper Methods //\n\n  /**\n   * Handle streaming response from AI\n   */\n  private async handleStreamingResponse(response: Response): Promise<string> {\n    const reader = response.body?.getReader()\n    if (!reader) {\n      throw new Error('No reader available for streaming response')\n    }\n\n    const decoder = new TextDecoder()\n    let fullText = ''\n\n    try {\n      // eslint-disable-next-line no-constant-condition\n      while (true) {\n        const { done, value } = await reader.read()\n        if (done) break\n\n        const chunk = decoder.decode(value, { stream: true })\n        // Process SSE format\n        const lines = chunk.split('\\n')\n        for (const line of lines) {\n          if (line.startsWith('data: ')) {\n            const data = line.slice(6)\n            if (data === '[DONE]') break\n            try {\n              const parsed = JSON.parse(data)\n              fullText += this.extractTextFromResponse(parsed)\n            } catch (e) {\n              // Skip invalid JSON\n            }\n          }\n        }\n      }\n    } finally {\n      reader.releaseLock()\n    }\n\n    return fullText\n  }\n\n  /**\n   * Extract text from various response formats\n   */\n  private extractTextFromResponse(response: any): string {\n    // Handle NonStreamPlayground format\n    if (response.text) {\n      return response.text\n    }\n\n    // Handle NonStreamChatbotResponse format\n    if (response.response) {\n      return response.response\n    }\n\n    // Handle GenericAPIResponse format\n    if (response.data && typeof response.data === 'string') {\n      return response.data\n    }\n\n    // Handle raw choices array\n    if (response.choices && Array.isArray(response.choices)) {\n      return (\n        response.choices[0]?.text || response.choices[0]?.message?.content || ''\n      )\n    }\n\n    // Fallback to string conversion\n    return String(response)\n  }\n  \n  /**\n   * Send a message and get AI response\n   * Handles both streaming and non-streaming responses\n   */\n  async sendMessage(\n    messages: Message[],\n    options?: Partial<I_InferenceGenerateOptions>\n  ): Promise<string> {\n    if (!this.isConnected()) {\n      throw new Error('Not connected to Obrew service')\n    }\n\n    // Create new abort controller for this request\n    this.abortController = new AbortController()\n\n    try {\n      const response = await this.connection?.api?.textInference.generate({\n        body: {\n          messages,\n          responseMode: 'chat',\n          temperature: 0.7,\n          max_tokens: 2048,\n          stream: false, // @TODO Non-streaming for now\n          ...options,\n        },\n        signal: this.abortController.signal,\n      })\n\n      if (!response) {\n        throw new Error('No response from AI service')\n      }\n\n      // Handle different response types\n      if (typeof response === 'string') {\n        return response\n      }\n\n      // Handle Response object (streaming)\n      // Check if it's a Response-like object with headers and body\n      if (\n        typeof response === 'object' &&\n        response !== null &&\n        'headers' in response &&\n        'body' in response\n      ) {\n        const httpResponse = response as Response\n        const contentType = httpResponse.headers.get('content-type')\n        if (contentType?.includes('event-stream')) {\n          // Handle streaming response\n          return await this.handleStreamingResponse(httpResponse)\n        } else {\n          // Handle JSON response\n          const data = await httpResponse.json()\n          return this.extractTextFromResponse(data)\n        }\n      }\n\n      // Handle structured response objects\n      return this.extractTextFromResponse(response)\n    } catch (error) {\n      if (error instanceof Error && error.name === 'AbortError') {\n        throw new Error('Request was cancelled')\n      }\n      throw error\n    }\n  }\n\n  /**\n   * Load a text model\n   */\n  async loadModel(modelPath: string, modelId: string): Promise<boolean> {\n    if (!this.isConnected()) {\n      throw new Error('Not connected to Obrew service')\n    }\n\n    try {\n      await this.connection?.api?.textInference.load({\n        body: {\n          modelPath,\n          modelId,\n          init: {\n            n_ctx: 4096,\n            n_threads: 4,\n            n_gpu_layers: -1,\n          },\n          call: {\n            temperature: 0.7,\n            max_tokens: 2048,\n          },\n        },\n      })\n      return true\n    } catch (error) {\n      console.error('Failed to load model:', error)\n      return false\n    }\n  }\n\n  /**\n   * Get currently loaded model info\n   */\n  async getLoadedModel() {\n    if (!this.isConnected()) {\n      return null\n    }\n\n    try {\n      const response = await this.connection?.api?.textInference.model()\n      return response?.data || null\n    } catch (error) {\n      console.error('Failed to get loaded model:', error)\n      return null\n    }\n  }\n\n  /**\n   * Get list of installed models\n   */\n  async getInstalledModels() {\n    if (!this.isConnected()) {\n      return []\n    }\n\n    try {\n      const response = await this.connection?.api?.textInference.installed()\n      return response?.data || []\n    } catch (error) {\n      console.error('Failed to get installed models:', error)\n      return []\n    }\n  }\n}\n\n// Export singleton instance\nexport const obrewClient = new ObrewClient()\n","/**\n * Type definitions for Obrew API\n * @module types\n */\n\n// ============================================================================\n// Message & Thread Types\n// ============================================================================\n\nexport type Message = {\n  id: string;\n  createdAt?: Date | undefined;\n  content: string;\n  role: \"system\" | \"user\" | \"assistant\";\n};\n\nexport interface I_Message {\n  id: string;\n  content: string;\n  role: \"system\" | \"user\" | \"assistant\";\n  createdAt?: string;\n  modelId?: string; // for assistant msg\n  username?: string; // for user msg\n}\n\nexport interface I_Thread {\n  id: string;\n  userId: string;\n  createdAt: string;\n  title: string;\n  summary: string;\n  numMessages: number;\n  messages: Array<I_Message>;\n  sharePath?: string;\n}\n\n// ============================================================================\n// Model Types\n// ============================================================================\n\nexport enum ModelID {\n  GPT3 = \"gpt3.5\",\n  GPT4 = \"gpt4\",\n  GPTNeo = \"gpt-neoxt-20B\", // together/\n  Cohere = \"xlarge\", // cohere/\n  Local = \"local\", // 3rd party, local server\n}\n\nexport type T_ModelConfig = {\n  repoId: string;\n  name: string;\n  description?: string;\n  messageFormat?: string;\n};\n\nexport interface I_ModelConfigs {\n  [key: string]: T_ModelConfig;\n}\n\nexport type T_InstalledTextModel = {\n  repoId: string;\n  savePath: { [key: string]: string };\n  numTimesRun: number;\n  isFavorited: boolean;\n  validation: string;\n  modified: string;\n  size: number;\n  endChunk: number;\n  progress: number;\n  tokenizerPath: string;\n  checksum: string;\n};\n\n// ============================================================================\n// LLM Configuration Types\n// ============================================================================\n\nexport interface I_LLM_Init_Options {\n  n_gpu_layers?: number;\n  use_mlock?: boolean;\n  seed?: number;\n  n_ctx?: number;\n  n_batch?: number;\n  n_threads?: number;\n  offload_kqv?: boolean;\n  cache_type_k?: string;\n  cache_type_v?: string;\n  verbose?: boolean;\n}\n\nexport interface I_Response_State {\n  temperature?: number;\n  max_tokens?: number;\n  top_p?: number;\n  echo?: boolean;\n  stop?: string;\n  repeat_penalty?: number;\n  top_k?: number;\n  stream?: boolean;\n  min_p?: number;\n  presence_penalty?: number; // 1.0\n  frequency_penalty?: number; // 1.0\n  tfs_z?: number;\n  mirostat_tau?: number;\n  grammar?: string;\n}\n\nexport interface I_LLM_Call_Options extends I_Response_State {\n  prompt?: string;\n  messages?: Message[];\n  suffix?: string;\n  model?: ModelID;\n  promptTemplate?: string;\n  systemMessage?: string;\n  response_mode?: string;\n}\n\nexport interface I_LLM_Options {\n  init?: I_LLM_Init_Options;\n  call?: I_LLM_Call_Options;\n}\n\n// ============================================================================\n// Conversation & Inference Types\n// ============================================================================\n\nexport const DEFAULT_CONVERSATION_MODE = \"instruct\";\nexport const DEFAULT_TOOL_RESPONSE_MODE = \"answer\";\nexport const BASE_RETRIEVAL_METHOD = \"base\";\nexport const AUGMENTED_RETRIEVAL_METHOD = \"augmented\";\nexport const AGENT_RETRIEVAL_METHOD = \"agent\";\nexport const DEFAULT_RETRIEVAL_METHOD = BASE_RETRIEVAL_METHOD;\nexport const NATIVE_TOOL_USE = \"native\";\nexport const UNIVERSAL_TOOL_USE = \"universal\";\nexport const DEFAULT_TOOL_USE_MODE = UNIVERSAL_TOOL_USE;\n\nexport type T_ConversationMode = \"instruct\" | \"chat\" | \"collab\";\nexport type T_ToolResponseMode = \"answer\" | \"result\";\nexport type T_ToolUseMode = typeof UNIVERSAL_TOOL_USE | typeof NATIVE_TOOL_USE;\nexport type T_ToolSchemaType = \"json\" | \"typescript\";\n\nexport interface I_InferenceGenerateOptions extends T_LLM_InferenceOptions {\n  responseMode?: T_ConversationMode;\n  toolResponseMode?: T_ToolResponseMode;\n  toolUseMode?: T_ToolUseMode;\n  messageFormat?: string;\n  memory?: I_Knowledge_State;\n  tools?: string[];\n}\n\nexport type T_LLM_InferenceOptions = I_LLM_Call_Options & I_LLM_Init_Options;\n\nexport interface I_LoadTextModelRequestPayload {\n  responseMode?: T_ConversationMode;\n  toolUseMode?: T_ToolUseMode;\n  toolSchemaType?: T_ToolSchemaType;\n  messages?: Message[];\n  raw_input?: boolean;\n  modelPath: string;\n  modelId: string;\n  init: I_LLM_Init_Options;\n  call: I_LLM_Call_Options;\n}\n\nexport interface I_LoadedModelRes {\n  modelId: string;\n  modelName: string;\n  responseMode: T_ConversationMode;\n  modelSettings: I_LLM_Init_Options;\n  generateSettings: I_LLM_Call_Options;\n}\n\n// ============================================================================\n// Response Types\n// ============================================================================\n\nexport interface I_NonStreamChatbotResponse {\n  metadata: { [key: string]: { order: number; sourceId: string } };\n  response: string;\n  source_nodes: Array<any>;\n}\n\nexport interface I_NonStreamPlayground {\n  additional_kwargs: any;\n  raw: {\n    choices: Array<any>;\n    created: number;\n    id: string;\n    model: string;\n    object: string;\n    usage: {\n      completion_tokens: number;\n      prompt_tokens: number;\n      total_tokens: number;\n    };\n  };\n  delta: number | null;\n  logprobs: any;\n  text: string;\n}\n\nexport interface I_GenericAPIResponse<DataResType> {\n  success: boolean;\n  message: string;\n  data: DataResType;\n}\n\n// ============================================================================\n// Request Types\n// ============================================================================\n\nexport type T_GenericDataRes = any;\nexport type T_GenericReqPayload = { [key: string]: any };\n\nexport interface I_GenericAPIRequestParams<Payload> {\n  queryParams?: Payload;\n  formData?: FormData;\n  body?: Payload;\n  signal?: AbortSignal;\n}\n\nexport type T_GenericAPIRequest<ReqPayload, DataResType> = (\n  props?: I_GenericAPIRequestParams<ReqPayload>\n) => Promise<I_GenericAPIResponse<DataResType> | null>;\n\nexport type T_SaveChatThreadAPIRequest = (props: {\n  body: {\n    threadId: string;\n    thread: I_Thread;\n  };\n}) => Promise<I_GenericAPIResponse<T_GenericDataRes>>;\n\nexport type T_GetChatThreadAPIRequest = (props: {\n  queryParams: {\n    threadId?: string | null;\n  };\n}) => Promise<I_GenericAPIResponse<I_Thread[]>>;\n\nexport type T_DeleteChatThreadAPIRequest = (props: {\n  queryParams: {\n    threadId?: string | null;\n  };\n}) => Promise<I_GenericAPIResponse<I_Thread[]>>;\n\n// ============================================================================\n// Knowledge & Memory Types\n// ============================================================================\n\nexport interface I_Knowledge_State {\n  ids: string[]; // collection names\n}\n\nexport interface I_RAG_Strat_State {\n  similarity_top_k: number;\n  response_mode: string | undefined;\n}\n\nexport interface I_ChunkMetadata {\n  _node_type: string;\n  _node_content: any;\n  sourceId: string;\n  ref_doc_id: string;\n  order: number;\n}\n\nexport interface I_Source {\n  id: string;\n  document_name: string;\n  embedding_model: string;\n  checksum: string;\n  urlPath: string;\n  source_file_name: string;\n  source_file_path: string;\n  file_path: string;\n  file_type: string;\n  file_name: string;\n  file_size: number;\n  modified_last: string;\n  created_at: string;\n  description: string;\n  tags: string;\n  chunkIds: Array<string>;\n}\n\nexport interface I_DocumentChunk {\n  text: string;\n  id: string;\n  metadata: I_ChunkMetadata;\n}\n\nexport interface I_Collection {\n  id: string;\n  name: string;\n  metadata: {\n    description: string;\n    embedding_model: string;\n    tags: string;\n    icon: string;\n    sources: Array<I_Source>;\n    created_at?: string;\n    sharePath?: string;\n    favorites?: number;\n    createdAt?: string;\n  };\n}\n\n// ============================================================================\n// Tool Types\n// ============================================================================\n\nexport type T_InputOptionTypes =\n  | \"options-sel\"\n  | \"options-multi\"\n  | \"options-button\"\n  | \"text\"\n  | \"text-multi\";\n\nexport type T_Tool_Param_Option = string[] | number[];\n\nexport interface I_Tool_Parameter {\n  name: string;\n  title: string;\n  description: string;\n  type: string;\n  placeholder?: string;\n  input_type?: T_InputOptionTypes;\n  default_value?: any;\n  value?: any;\n  min_value?: string | number;\n  max_value?: string | number;\n  options_source?: string;\n  options_description?: string[];\n  options?: string[];\n  items?: any[];\n}\n\nexport interface I_Tool_Def_Parameter extends I_Tool_Parameter {\n  value?: any;\n}\n\nexport interface I_ToolFunctionSchemaRes {\n  params: I_Tool_Parameter[];\n  description?: string | undefined;\n  params_schema?: any | undefined;\n  params_example?: any | undefined;\n  output_type?: string[];\n}\n\nexport interface I_Tool_Definition extends I_ToolFunctionSchemaRes {\n  name: string;\n  path: string;\n  id?: string | undefined; // assigned on tool save\n}\n\n// ============================================================================\n// Settings Types\n// ============================================================================\n\nexport type T_PromptTemplate = {\n  id: string;\n  name: string;\n  text: string;\n};\n\nexport type T_SystemPrompt = {\n  id: string;\n  name: string;\n  text: string;\n};\n\nexport interface I_PromptTemplates {\n  [key: string]: T_PromptTemplate[];\n}\n\nexport type T_SystemPrompts = {\n  presets: { [key: string]: T_SystemPrompt[] };\n};\n\nexport type I_Prompt_State = {\n  promptTemplate: T_PromptTemplate;\n};\n\nexport interface I_Model_State {\n  id: string | undefined;\n  botName?: string;\n  filename: string | undefined;\n}\n\nexport interface I_System_State {\n  systemMessage: string | undefined;\n  systemMessageName: string | undefined;\n}\n\nexport interface I_Attention_State {\n  tool_use_mode: T_ToolUseMode;\n  tool_response_mode: T_ToolResponseMode;\n  response_mode: T_ConversationMode;\n}\n\nexport interface I_Tools_Inference_State {\n  assigned: string[];\n}\n\nexport interface I_Text_Settings {\n  tools: I_Tools_Inference_State;\n  attention: I_Attention_State;\n  performance: I_LLM_Init_Options;\n  system: I_System_State;\n  model: I_Model_State;\n  prompt: I_Prompt_State;\n  response: I_Response_State;\n  memory: I_Knowledge_State;\n}\n\n// ============================================================================\n// API Configuration Types\n// ============================================================================\n\nexport interface I_ConnectionConfig {\n  domain: string\n  port: string\n  version: string\n  enabled: boolean\n}\n\nexport interface I_Connection {\n    config: I_ConnectionConfig,\n    api: I_ServiceApis | null\n}\n\nexport type T_APIConfigOptions = {\n  chunkingStrategies?: Array<string>;\n  domain?: string;\n  port?: string;\n};\n\nexport interface I_Endpoint {\n  name: string;\n  urlPath: string;\n  method: string;\n}\n\nexport interface I_API {\n  name: string;\n  port: number;\n  endpoints: Array<I_Endpoint>;\n  configs?: T_APIConfigOptions;\n}\n\nexport interface I_ServicesResponse {\n  success: boolean;\n  message: string;\n  data: Array<I_API>;\n}\n\nexport interface I_ConnectResponse {\n  success: boolean;\n  message: string;\n  data: { docs: string };\n}\n\n// ============================================================================\n// Service API Types\n// ============================================================================\n\nexport type T_Endpoint = { [key: string]: any };\n\nexport interface I_BaseServiceApis {\n  [key: string]: T_Endpoint;\n}\n\nexport type T_TextInferenceAPIRequest = (props: {\n  body: I_InferenceGenerateOptions;\n  signal: AbortSignal;\n}) => Promise<\n  | Response\n  | I_NonStreamPlayground\n  | I_NonStreamChatbotResponse\n  | string // a JSON string\n  | I_GenericAPIResponse<any>\n  | null\n>\n\nexport interface I_DeleteTextModelReqPayload {\n  repoId: string;\n  filename: string;\n}\n\nexport interface I_ToolSchemaReqPayload {\n  filename: string;\n}\n\nexport interface I_ServiceApis extends I_BaseServiceApis {\n  /**\n   * Use to query the text inference engine\n   */\n  textInference: {\n    generate: T_TextInferenceAPIRequest;\n    stop: T_GenericAPIRequest<T_GenericReqPayload, T_GenericDataRes>;\n    load: T_GenericAPIRequest<\n      I_LoadTextModelRequestPayload,\n      I_GenericAPIResponse<undefined>\n    >;\n    unload: T_GenericAPIRequest<T_GenericReqPayload, T_GenericDataRes>;\n    model: T_GenericAPIRequest<T_GenericReqPayload, I_LoadedModelRes>; // Currently loaded text model\n    modelExplore: T_GenericAPIRequest<T_GenericReqPayload, T_GenericDataRes>;\n    installed: T_GenericAPIRequest<T_GenericReqPayload, T_InstalledTextModel[]>; // List of currently installed text models\n    getModelMetadata: T_GenericAPIRequest<\n      T_GenericReqPayload,\n      T_GenericDataRes\n    >;\n    getModelInfo: T_GenericAPIRequest<T_GenericReqPayload, T_GenericDataRes>;\n    download: T_GenericAPIRequest<T_GenericReqPayload, string>;\n    delete: T_GenericAPIRequest<I_DeleteTextModelReqPayload, T_GenericDataRes>;\n    getModelConfigs: T_GenericAPIRequest<T_GenericReqPayload, T_GenericDataRes>;\n    // getPromptTemplates: T_GenericAPIRequest<\n    //   T_GenericReqPayload,\n    //   T_GenericDataRes\n    // >;\n    // getSystemPrompts: T_GenericAPIRequest<\n    //   T_GenericReqPayload,\n    //   T_GenericDataRes\n    // >;\n  };\n  /**\n   * Use to add/create/update/delete embeddings from database\n   */\n  memory: {\n    addDocument: T_GenericAPIRequest<T_GenericReqPayload, T_GenericDataRes>;\n    getChunks: T_GenericAPIRequest<T_GenericReqPayload, T_GenericDataRes>;\n    updateDocument: T_GenericAPIRequest<T_GenericReqPayload, T_GenericDataRes>;\n    deleteDocuments: T_GenericAPIRequest<T_GenericReqPayload, T_GenericDataRes>;\n    getAllCollections: T_GenericAPIRequest<\n      T_GenericReqPayload,\n      T_GenericDataRes\n    >;\n    addCollection: T_GenericAPIRequest<T_GenericReqPayload, T_GenericDataRes>;\n    getCollection: T_GenericAPIRequest<T_GenericReqPayload, I_Collection>;\n    deleteCollection: T_GenericAPIRequest<\n      T_GenericReqPayload,\n      T_GenericDataRes\n    >;\n    fileExplore: T_GenericAPIRequest<T_GenericReqPayload, T_GenericDataRes>;\n    wipe: T_GenericAPIRequest<T_GenericReqPayload, T_GenericDataRes>;\n    configs: {\n      chunkingStrategies: Array<string>;\n    };\n  };\n  /**\n   * Use to persist data specific to the app itself\n   */\n  storage: {\n    getToolSchema: T_GenericAPIRequest<\n      I_ToolSchemaReqPayload,\n      I_ToolFunctionSchemaRes\n    >;\n    getToolFunctions: T_GenericAPIRequest<\n      T_GenericReqPayload,\n      T_GenericDataRes\n    >;\n    saveToolSettings?: T_GenericAPIRequest<T_GenericReqPayload, null>;\n    getToolSettings?: T_GenericAPIRequest<\n      T_GenericReqPayload,\n      I_Tool_Definition[]\n    >;\n    deleteToolSettings?: T_GenericAPIRequest<T_GenericReqPayload, null>;\n    getBotSettings: T_GenericAPIRequest<T_GenericReqPayload, I_Text_Settings[]>;\n    deleteBotSettings: T_GenericAPIRequest<\n      T_GenericReqPayload,\n      I_Text_Settings[]\n    >;\n    saveBotSettings: T_GenericAPIRequest<\n      T_GenericReqPayload,\n      I_Text_Settings[]\n    >;\n    saveChatThread: T_SaveChatThreadAPIRequest;\n    getChatThread: T_GetChatThreadAPIRequest;\n    deleteChatThread: T_DeleteChatThreadAPIRequest;\n  };\n}\n"]}